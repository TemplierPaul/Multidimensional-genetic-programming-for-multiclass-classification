{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional genetic programming for multiclass classification\n",
    "\n",
    "This notebook is based on:  \n",
    "__W. La Cava, S. Silva, K. Danai, L. Spector, L. Vanneschi, J.H. Moore,   \n",
    "Multidimensional genetic programming for multiclass classification,   \n",
    "Swarm and Evolutionary Computation BASE DATA (2018),  \n",
    "doi: 10.1016/j.swevo.2018.03.015.__   \n",
    "Article available at: https://www.sciencedirect.com/science/article/abs/pii/S2210650217309136?via%3Dihub  \n",
    "\n",
    "Some explanation parts of this notebook may directly quote or paraphrase this article or other sources for clarity purposes.\n",
    "\n",
    "The included code has been developed for the ISAE-Supaero SDD evaluation in order to showcase how genetic programming can be used for feature engineering. \n",
    "\n",
    "__Author__: Student 57 (anonimized)  \n",
    "__License__: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements \n",
    "The following libraries for Python 3.6+ are required:\n",
    "- numpy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- sklearn\n",
    "- seaborn \n",
    "\n",
    "If they are not installed, run:   \n",
    "`!pip install numpy matplotlib seaborn pandas sklearn`   \n",
    "`!pip update numpy matplotlib seaborn pandas sklearn`\n",
    "\n",
    "The code for this notebook is stored in `./Code` and loaded when needed.  \n",
    "When you encounter a cell with `%load Code/name.py`, run it twice: once to load the code, once to execute it.  \n",
    "If the `%load` command is commented, this is meant to be a solution to the question. Uncomment it and run it twice as well to load the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.base import clone\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>In a nutshell:</b>\n",
    "<ul>\n",
    "<li> Feature Selection and Feature Construction can create a new feature space where the samples are more accurately classified by a Nearest Centroid model\n",
    "<li> Genetic Programming evolves transformations of the feature space to select the ones giving the best classification results \n",
    "<li> Evaluating intelligibility as well as performance in the fitness calculation allows to maintain good performance while avoiding black-box models like neural networks\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Feature Selection and Intelligibility\n",
    "\n",
    "Feature selection and feature construction play fundamental roles in the application of machine learning (ML) to classiﬁcation. Feature selection makes it possible, for example, to reduce high-dimensional datasets to a manageable size, and to reﬁne experimental designs through measurement selection in some domains. The ML community has become increasingly aware of the need for automated and ﬂexible feature engineering methods to complement the large set of classiﬁcation methodologies that are now widely available in opensource packages such as Weka and Scikit-Learn.  \n",
    "\n",
    "Typical classiﬁcation pipelines treat feature selection and feature construction as pre-processing steps, in which the attributes in the dataset are selected according to some heuristic and then projected into more complex feature spaces using e.g. kernel functions. In both cases the feature pre-processing is often conducted in a trial-and-error way rather than being automated or intrinsic to the learning method. The use of non-linear feature expansions can also lead to classiﬁers that are black-box, making it diﬃcult for researchers to gain insight into the modelled process by studying the model itself. \n",
    "\n",
    "This paper investigates a multiclass classiﬁcation strategy designed to integrate feature selection, construction and model intelligibility goals into a distance-based classiﬁer to improve its ability to build accurate and simple classiﬁers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We introduce 2 datasets for multiclass classification.  \n",
    "Each is imported and converted into a pandas.DataFrame, and a 10% validation set is kept for the final selection. \n",
    "\n",
    "### Iris\n",
    "\n",
    "The first toy dataset we can use for multiclass classification is the very well known `Iris dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_load = datasets.load_iris()\n",
    "iris = pd.DataFrame(iris_load['data'], columns=['sep_l', 'sep_w', 'pet_l', 'pet_w'])\n",
    "iris['target']=iris_load['target']\n",
    "iris, iris_val = train_test_split(iris, test_size=0.1)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Localization Sites\n",
    "\n",
    "From: https://archive.ics.uci.edu/ml/datasets/Yeast  \n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "This dataset, also called `Yeast dataset`, has been created in 1996. Its goal is to use statistical modeling to predict the cellular localization of proteins from their attributes.\n",
    "\n",
    "__Results__: __55%__ for Yeast data with an ad hoc structured probability model. \n",
    "Also similar accuracy for Binary Decision Tree and Bayesian Classifier methods applied by the same authors in unpublished results.\n",
    "\n",
    "\n",
    "Features in this dataset:\n",
    "-  Sequence Name: Accession number for the SWISS-PROT database\n",
    "-   mcg: McGeoch's method for signal sequence recognition.\n",
    "-   gvh: von Heijne's method for signal sequence recognition.\n",
    "-   alm: Score of the ALOM membrane spanning region prediction program.\n",
    "-   mit: Score of discriminant analysis of the amino acid content of the N-terminal region (20 residues long) of mitochondrial and non-mitochondrial proteins.\n",
    "-   erl: Presence of \"HDEL\" substring (thought to act as a signal for retention in the endoplasmic reticulum lumen). Binary attribute.\n",
    "-   pox: Peroxisomal targeting signal in the C-terminus.\n",
    "-   vac: Score of discriminant analysis of the amino acid content of vacuolar and extracellular proteins.\n",
    "-   nuc: Score of discriminant analysis of nuclear localization signals of nuclear and non-nuclear proteins.\n",
    "  \n",
    "The *Sequence Name* attribute is dropped as it is unique to each data point.\n",
    "The predicted class is encoded in the *target* column to fit the rest of the code.\n",
    "\n",
    "For more information, run `%load Data/yeast.names` in a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast = pd.read_csv('Data/yeast.data', sep='\\s+', header=None)\n",
    "yeast.columns = ['Sequence_name', 'MCG', 'GVH', 'ALM', 'MIT', 'ERL', 'POX', 'VAC', 'NUC', 'target']\n",
    "yeast['target'] = yeast['target'].astype('category').cat.codes\n",
    "yeast = yeast.drop(columns=['Sequence_name', 'POX'])\n",
    "yeast, yeast_val = train_test_split(yeast, test_size=0.1)\n",
    "yeast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1335, 8)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Switch between yeast and iris to choose the one you want to use\n",
    "data = yeast \n",
    "\n",
    "if data is yeast:\n",
    "    data_val = yeast_val\n",
    "elif data is iris:\n",
    "    data_val = iris_val\n",
    "    \n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>For later use</b><br>\n",
    "    You can use this notebook with your own data to automate feature engineering for your classification projects! <br>\n",
    "    Import your dataset as <code>data</code>, with a <code>target</code> column for the classes to predict, and your validation set as <code>data_val</code>.<br>\n",
    "    Or indicate the path to you csv in the next cell.<br><br>\n",
    "    Then, run the notebook!\n",
    "</div>\n",
    "\n",
    "(You may have to pre-process your dataset to avoid unforseen bugs and/or conflicts with the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = #TODO\n",
    "df = pd.read_csv(path)\n",
    "data, data_val = train_test_split(df, test_size=0.1)\n",
    "\n",
    "# Checking the data format \n",
    "assert 'target' in data.columns\n",
    "for c in data.columns:\n",
    "    assert type(data[c]) is not str\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Code</b><br>\n",
    "    Explore the <code>yeast</code> dataset. <br>\n",
    "    Do you see any class easily separable from the rest?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = 0, 1\n",
    "col = data.columns\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "scatter = ax.scatter(data[col[i]], data[col[j]], c=data['target'])\n",
    "plt.xlabel(col[i])\n",
    "plt.ylabel(col[j])\n",
    "legend1 = ax.legend(*scatter.legend_elements(),\n",
    "                    loc=\"lower right\", title=\"Classes\")\n",
    "ax.add_artist(legend1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "The goal of multiclass classiﬁcation is to ﬁnd a mapping between any vector $x \\in \\mathbb{R}^p $ and a class label from the set $C = \\{1 ... K\\}$, with $K > 2$:\n",
    "\n",
    "$$ \\hat{y}(x) : \\mathbb{R}^p \\to C $$\n",
    "\n",
    "using a training set $T = \\{(x_i,y_i),i = 1 ... n\\}$ that associates each example $x_i \\in \\mathbb{R}^p $ to a label $y_i \\in C$.\n",
    "\n",
    "The *Nearest Centroid* algorithm conducts classiﬁcation by measuring the similarity of each vector to the bulk properties of the examples within each class, and then assigning the label corresponding to the most similar group.    \n",
    "The attributes are partitioned into subsets $\\{X_1 ... X_K\\}$, such that $X_k$ is the subset of x with class label k:\n",
    "$$X_k = \\{x_i | y_i == k\\}$$\n",
    "\n",
    "\n",
    "To classify a new sample $x' \\in \\mathbb{R}^p$, the distance of $x'$ to each subset $\\{X_1 ... X_K\\}$ is measured and the class label corresponding to the minimum distance is assigned to $x'$:  \n",
    "$$ \\hat{y}(x') = j, \\:where\\: j= \\underset{k}{argmin}\\: D(x', X_k) $$\n",
    "\n",
    "$D$ being a distance operator between $x'$ and the centroid of $X_k$.  \n",
    "Previous work indicates the [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance) to perform better on classification than the Euclidean distance.  \n",
    "\n",
    "We compute the mean accuracy on the given test data and labels.   \n",
    "In multi-label classification, this is the subset accuracy, indicating the percentage of samples that have all their labels classified correctly.\n",
    "\n",
    "__Note:__\n",
    "Since this notebook will rely on the `sklearn` library to perform Nearest Centroid classification, the sensitivity of the matrix inversion (used in the computation of the Mahalanobis distance) in the `numpy` library to any linear combination of features will lead us to use Euclidean distance in the implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(data, target='target'):\n",
    "    df = data.drop(columns=['target'])\n",
    "    model = NearestCentroid(metric='euclidean')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, \n",
    "                                                    data[target], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = compute_score(data, target='target')\n",
    "print(\"Score:\", s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformation\n",
    "\n",
    "### Limits of Nearest Centroid classification\n",
    "\n",
    "<img src=\"img/example.png\" width=\"800\">\n",
    "<center><a href='#Sources'>Source: Multidimensional genetic programming for multiclass classification</a></center>\n",
    "\n",
    "The nearest centroid classiﬁer assumes the within-class samples to be normally distributed about their centroid and separated from the distributions of other classes. For this reason, the data in the right plot is easily classiﬁable using nearest centroids. Unfortunately, many real-world problems have data that violates these assumptions, as in the left plot.  \n",
    "Furthermore, for high dimensional data, calculating the distance between a point and a centroid can be impractical. However, by ﬁnding a set of transformations that project the data into a space that delineates class membership, the performance of a nearest centroid classiﬁer can be improved and high-dimensional distance comparisons may be avoided. Furthermore, the relationship between the raw data and the transformed space can be made understandable by using symbolic transformations, as in $\\phi_1$ and $\\phi_2$. \n",
    "\n",
    "The goal of the algorithm described in this paper is to ﬁnd a set of transformations \n",
    "$\\phi(x) : \\mathbb{R}^p \\to \\mathbb{R}^d $\n",
    "that projects $x$ into a space in which the samples are more accurately classiﬁed by the nearest centroid method. \n",
    "\n",
    "Classiﬁcation is then conducted with centroids $\\mu_{\\phi_k} \\in \\mathbb{R}^d$ and distances $D(\\phi(x'), \\phi(X_k))$.\n",
    "Our goal will be to approximate the optimal synthesized features $\\phi^* = [\\phi_1 ... \\phi_d]$ that maximize the number of correctly classiﬁed training samples, as:\n",
    "\n",
    "$$ \\phi^*(x) = \\underset{\\phi \\in \\mathbb{S}}{argmax}\\: f(\\phi, \\mathcal{T})$$\n",
    "\n",
    "$$ f(\\phi, \\mathcal{T}) = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}(\\hat{y}(\\phi(x_i))=y_i)$$ \n",
    "\n",
    "where $\\mathbb{S}$ is the space of possible transformations $\\phi$, $f$ is the classiﬁcation accuracy, and the indicator function  $\\mathbb{I} = 1$ if $\\hat{y}(\\phi(x_i))=y_i$, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Implementation*: `Var` class\n",
    "\n",
    "In order to apply operations to features more easily, we define a `Var` object.\n",
    "A `Var` object represents a feature or combination of features through a basic operation.\n",
    "Hence, it can be of 3 types:\n",
    "- feature from the dataset\n",
    "- 1-parameter function of another `Var` object: $sin, cos, exp$\n",
    "- 2-parameters function of 2 other `Var` objects: $+, -, \\times, \\div $\n",
    "\n",
    "The last 2 categories are summed up in the `Var.FUNCTIONS` dictionaries linking function names to functions.\n",
    "- `FUNCTIONS[1]` lists 1-parameter functions\n",
    "- `FUNCTIONS[2]` lists 2-parameters functions\n",
    "\n",
    "If an element is in neither of them, it is considered as a feature from the dataset.\n",
    "\n",
    "__Run the following cell to load the code, then run it again to use it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/var.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $sin, cos$ and $exp$ are defined in the `math` library, we extend their range thanks to a wrapper.\n",
    "If one of these functions is called on a `Var` object, it will call the method from the `Var` class. On anything else, it will proceed as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Sin, Cos and Exp are redefined to be applicable on Var objects\n",
    "def wrap_function(func):\n",
    "    def wrapper(x):\n",
    "        if type(x) is Var:\n",
    "            if func.__name__ == 'sin':\n",
    "                result = x.sin()\n",
    "            elif func.__name__ == 'cos':\n",
    "                result = x.cos()\n",
    "            elif func.__name__ == 'exp':\n",
    "                result = x.exp()\n",
    "            elif func.__name__ == 'log':\n",
    "                result = x.log()\n",
    "        else: \n",
    "            result = func(x)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "sin = wrap_function(math.sin)\n",
    "cos = wrap_function(math.cos)\n",
    "exp = wrap_function(math.exp)\n",
    "log = wrap_function(math.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any basic operation can be done on `Var`objects easily.\n",
    "\n",
    "Ex:<code>  \n",
    "length = Var(name='length')  \n",
    "width  = Var(name='width')  \n",
    "area   = length * width  \n",
    "</code> \n",
    "\n",
    "`area` is a `Var` and can be used as such\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Var.compute(data)` computes the column of the new dataset corresponding to the operation defined by this `Var` from the initial dataset `data`.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Code</b><br>\n",
    "Create, compute and plot the following features for the <code>Iris</code> dataset with this class:\n",
    "<ul>\n",
    "<li> sepal length + petal length\n",
    "<li> petal length * petal width\n",
    "<li> sin(exp(petal length))\n",
    "</ul>\n",
    "</div>\n",
    "Remember: the column names are: 'sep_l', 'sep_w', 'pet_l', 'pet_w'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load the solution\n",
    "#%load Code/sol_iris.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Programming\n",
    "\n",
    "Genetic programming is an evolutionary computation technique where a population of computer programs or functions are evolved. Generation by generation, Genetic Programming stochastically transforms populations of programs into new, hopefully better, populations of programs.\n",
    "\n",
    "It is a variant to Genetic Algorithms since it optimizes programs to find the optimum of the fitness function, designed as a measure of the efficiency of a solution to solve the problem at hand.   \n",
    "As it is a stochastic (ie random) process results cannot be quaranteed, but it allows to escape traps in which deterministic methods may fall. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program representation\n",
    "\n",
    "Each program $\\phi(x) : \\mathbb{R}^p \\to \\mathbb{R}^d $ can be seen as a set of equations. For example, an individual program $i$ might encode the features:\n",
    "\n",
    "$$ i \\to \\phi(x) = [x_1, x_2, x_1^2, x_2^2, x_1 x_2]$$\n",
    "\n",
    "\n",
    "This work implements a stack-based representation of the equations that create the new features. Programs in this representation are encoded as post-ﬁx notation equations, e.g.:\n",
    "$ i = [x_1, x_2, +] \\to \\phi = [x_1 + x_2]$  \n",
    "The $i$ representation is flattened: $ \\phi = [(x_1 + x_2)*x_3] \\to i = [x_1, x_2, +, x_3, *] $  \n",
    "\n",
    "Programs are evaluated via executions on a stack, such that the program $i$ above can be constructed as\n",
    "$$i = [ x_1,  x_2,  x_1,  x_1,  ∗,  x_2,  x_2,  ∗,  x_1,  x_2,  ∗ ]$$\n",
    "\n",
    "The execution of program $i$ is illustrated in this figure:\n",
    "\n",
    "<img src=\"img/Multidimensional_transformation.png\" width=\"800\">\n",
    "<center><a href='#Sources'>Source: Multidimensional genetic programming for multiclass classification</a></center>\n",
    "\n",
    "Stack based evaluation proceeds left to right, pushing and pulling instructions to and from a single stack. Arguments such as $x_1$ are pushed to the stack, and operators such as $*$ pull arguments from the stack and push the result. At the end of a program’s execution, the entire stack represents the multi-dimensional transformation.\n",
    "\n",
    "#### Note - Implementation\n",
    "In this implementation and to prevent infinite or `NaN` values in the dataset, operations have been restrained to:\n",
    "$$+, -, *, sin, cos$$\n",
    "\n",
    "Other operators such as $\\div, log$ and $exp$ have been implemented but were not enabled as they either cannot compute null or negative values, or their output can outrange the machine capacity.  \n",
    "\n",
    "To enable them, modify the `Var.FUNCTIONS` dictionary (not recommended in this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating instructions\n",
    "We define 2 lists of operators in the FUNCTIONS dictionary.\n",
    "\n",
    "`FUNCTIONS = {\n",
    "        1:['sin', 'cos', 'exp'],\n",
    "        2:['+', '-', '*', '/']\n",
    "    }\n",
    "`\n",
    "\n",
    "`FUNCTIONS [1]` lists functions that require 1 argument.   \n",
    "`FUNCTIONS [2]` lists functions that require 2 arguments.\n",
    "\n",
    "With `a` and `b` as variable names, a program of size 7 may look like the following list:\n",
    "`['a', 'a', '+', 'b', 'sin', 'a', '*']`  \n",
    "which once executed gives the following features list:\n",
    "$$[a + a, \\:\\sin(b) * a]$$\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Code</b><br>\n",
    "Complete next cell to generate random lists of instructions of size n from any list of variable names passed in argument.<br> \n",
    "What are the constraints linked to the number of arguments a function needs? <br>\n",
    "Execute your program manually to separate the features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(size, variables):\n",
    "    \"\"\"\n",
    "    Return a random list of instructions.\n",
    "\n",
    "    :param: size Int\n",
    "        Number of instructions in the list\n",
    "    :param: list of String\n",
    "        List of the names of the variables to use to generate the instructions\n",
    "    \"\"\"\n",
    "    FUNCTIONS = {\n",
    "        1:['sin', 'cos', 'exp'],\n",
    "        2:['+', '-', '*', '/']\n",
    "    }\n",
    "    \n",
    "    #TODO\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load the solution\n",
    "# %load Code/generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "v =['a', 'b', 'c']\n",
    "\n",
    "l = generate(size=10, variables=v)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting features\n",
    "\n",
    "In order to do mutation and crossover operations on the features, we need to split the list of instructions into sub-lists of 1 feature each.  \n",
    "Thus, we define a `split_features` method to return a list of features, aggregating the instructions in each of the root where they belong.\n",
    "\n",
    "__Run the following cell to load the code, then run it again to use it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/split_features.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Code</b><br>\n",
    "    Generate a random instructions list with the code from the previous part and split it with this new method. <br>\n",
    "    Is the result what you expected? If not, edit your <code>generate()</code> method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "l = generate(size=10, variables=v)\n",
    "print(l)\n",
    "split_features(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on trees\n",
    "\n",
    "Each new feature can be seen as a tree: \n",
    "\n",
    "<img src=\"img/tree.png\" width=\"200\">\n",
    "<center><a href='#Sources'>Source: A Field Guide to Genetic Programming</a></center>\n",
    "\n",
    "This tree implements the $x + 3 * y$ function, and can be represented as a stack:\n",
    "$$i=[x, 3, y, *, +]$$\n",
    "\n",
    "Hence, any genetic operators widely used on trees in Genetic Programming can be used to define Mutation and Crossover on the instruction lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutation\n",
    "\n",
    "<img src=\"img/tree_mutation.png\" width=\"400\">\n",
    "<center><a href='#Sources'>Source: A Field Guide to Genetic Programming</a></center>\n",
    "\n",
    "The mutation operator, with equal probability, chooses one of three actions: \n",
    "- it replaces a sub-tree with a randomly generated sub-tree\n",
    "- it adds a new randomly generated sub-tree to the end of the program, thereby increasing the number of features by one\n",
    "- it deletes a sub-tree corresponding to a “root”, thereby reducing the number of features by one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossover\n",
    "\n",
    "<img src=\"img/tree_crossover.png\" width=\"400\">\n",
    "<center><a href='#Sources'>Source: A Field Guide to Genetic Programming</a></center>\n",
    "\n",
    "The crossover operator similarly chooses between one of two equally probable actions: \n",
    "- it performs standard sub-tree crossover of the parents, selecting non-“root” nodes\n",
    "- it performs standard sub-tree crossover of “root” nodes.   \n",
    "\n",
    "In this case, “roots” are those nodes in the program that produce a value in the ﬁnal stack, and can be identiﬁed from stack-based programs in linear time.  \n",
    "\n",
    "2 children are often generated during crossover by switching parts of the parents' trees. They are thus complementary wrt. their parents, hence we only keep one of them in the new population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Program` class\n",
    "\n",
    "A `Program` object contains a list of instructions and all the related tools to the transformation of features on a dataset.\n",
    "\n",
    "__Run the following cell to load the code, then run it again to use it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/programs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Code</b><br>\n",
    "    In the next cell we generate random <code>Program</code> objects for the <code>yeast</code> dataset, then make them mutate and crossover. <br>\n",
    "    What type of mutation happened? <br>\n",
    "    What type of crossover happened? <br>\n",
    "    Can you retrace which parent each feature came from in the children?<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Var definition for each column except 'target'\n",
    "v = {}\n",
    "for i in data.columns[:-1]:\n",
    "    v[i] = Var(name=i)\n",
    "print(\"Variables:\", list(v.keys()), '\\n')\n",
    "\n",
    "# Mutation\n",
    "a = Program(v).generate(20).get_features()\n",
    "print('>> A\\n', a)\n",
    "\n",
    "a.mutate()\n",
    "print('>> A after mutation\\n', a)\n",
    "\n",
    "# Crossover: parents\n",
    "p1 = Program(v).generate(10).get_features()\n",
    "print('>> P1\\n', p1)\n",
    "\n",
    "p2 = Program(v).generate(10).get_features()\n",
    "print('>> P2\\n', p2)\n",
    "\n",
    "# Children\n",
    "c1, c2 = p1.crossover(p2)\n",
    "print('>> Children:\\n', c1, '\\n', c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population creation and Generation cycles\n",
    "\n",
    "The population of the Genetic Algorithm (GA) is initialized as an array of `Program` object, each randomly generated.  \n",
    "At each generation, the population is evaluated and 3 genetic operators are applied to form the new population:\n",
    "- Mutation: individuals are selected, mutated, and added to the new population\n",
    "- Crossover: pairs of individuals are selected from the population, and their features are merged to create 2 children, between which the parents' features are divided. One child is added to the new population\n",
    "- Reproduction: if the crossover and mutation operations do not create a list with the same length as the previous population, the best individuals are copied into the new population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tournament selection\n",
    "\n",
    "Tournament selection is a selection operator commonly used in Genetic Algorithms to select which individuals will be used in each of the genetic operators described above. \n",
    "\n",
    "Every time a new element needs to be picked - either to be mutated or to be used as a parent in a crossover -:\n",
    "- $\\lambda$ individuals are selected at random\n",
    "- among them, the one with the highest fitness is chosen.\n",
    "\n",
    "Here, the tournament only decides between 2 individuals ($\\lambda = 2$). A higher $\\lambda$ can be chosen for higer selection pressure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Population` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/population.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Code</b><br>\n",
    "    Test the <code>Population</code> class on the <code>Yeast</code> dataset and look through `p.archive`. <br>\n",
    "    Is the maximum fitness improving? <br>\n",
    "    Which solution would you recommend?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "p = Population(data = yeast, init_size=15, pop_size=100)\n",
    "p.generation(n=20, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Selection\n",
    "\n",
    "#### Reminder: Pareto Dominance\n",
    "When trying to minimize $f_1$ and $f_2$, $x_1$ dominates $x_2$ if:\n",
    "\n",
    "$$\\forall i \\in \\{1, 2\\}, f_i(x_1) \\leq f_i(x_2) $$\n",
    "<center>and</center> $$\\exists j \\in \\{1, 2\\}, f_i(x_1) < f_i(x_2)$$\n",
    "\n",
    "The Pareto front is defined as the set of non-dominated points:\n",
    "\n",
    "<img src=\"img/pareto.png\" width=\"400\">\n",
    "\n",
    "#### Archiving and final selection\n",
    "\n",
    "Typically in GP the individual with the highest training accuracy (the so-called ‘best-of-run’ individual) is chosen as the ﬁnal model and evaluated on the test set.   \n",
    "However, given the importance of interpretability, on subsequent problems we maintain an archive of solutions that represent the best trade-oﬀs of training accuracy and complexity (i.e. the Pareto set). To choose a ﬁnal model, a small validation set `data_val` is split from the training set before the run. At the end of the run, the archive is evaluated on the validation set, and the individual with the best score is chosen as the ﬁnal model. This approach should guard against over-ﬁtting and produce simpler models. The entire archive can also be studied by experts to get a better sense of the building blocks of good solutions to their problem, which can oﬀer insight to the user.\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Question</b><br>\n",
    "    How did each solution in the archive adapt to the validation set? <br>\n",
    "    Would you still recommend the same solution?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.final_selection(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other selection methods\n",
    "\n",
    "### Lexicase Selection\n",
    "In each parent selection event, the lexicase selection algorithm ﬁrst randomly orders the test cases. It then eliminates any individuals in the population that do not have the best performance on the ﬁrst test case. Assuming that more than one individual remains, it then loops, eliminating any individuals from the remaining candidates that do not have the best performance on the second test case. This process continues until only one individual remains and is selected, or until all test cases have been used, in which case it randomly selects one of the remaining individuals. \n",
    "\n",
    "Lexicase selection sometimes selects individuals that perform well on a relatively small number of test cases. This differs from most other selection algorithms, which select individuals based on aggregations of performance on all test cases. Lexicase selection may select individuals that perform very poorly on some test cases if they excel on a combination of others. As such, lexicase often selects “specialist” individuals that solve parts of the problem extremely well. Although these individuals may have worse summed error across all test cases, the hope is they will be able to reproduce in ways that pass on their preeminence on certain cases while improving with respect to others. In order to give every test case equal selection pressure, each lexicase selection event uses a randomly shufﬂed list of test cases to determine which test cases are treated as most important. \n",
    "\n",
    "#### Implementation\n",
    "\n",
    "We use inheritance to define a new class `LexPopulation` from `Population` that will implement lexicase selection instead of tournament selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/lexicase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next cell to perform the process with Lexicase Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = LexPopulation(data = yeast, init_size=15, pop_size=100)\n",
    "p.generation(n=20, plot=False)\n",
    "p.final_selection(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age-Fitness Pareto survival\n",
    "\n",
    "#### Non-Dominated Sorting\n",
    "\n",
    "Non-Dominated Sorting (NDS) is an algorithm that computes a fitness from the number of other points dominated by each individual.  \n",
    "NDS maps the solutions $x_i$ of a population $P$ to $K$ Pareto Fronts $F_1, ..., F_K$.   \n",
    "It first selects all the non-dominated solutions from $P$ and assigns them to $F_1$ (rank 1). It then selects all the non-dominated solutions from the remaining population and assigns them to $F_2$ (the rank 2 Pareto front).   \n",
    "This process is repeated untill all solutions have been assigned to a front.\n",
    "\n",
    "<img src=\"img/nds.png\" width=\"400\">\n",
    "\n",
    "The fitness is then computed from the number of individuals dominated by $x_i$ divided by the size of the global population:\n",
    "$$F(x_i) = \\frac{Number \\: of\\:solutions \\: dominated \\:by\\:x_i}{Total\\: number\\: of\\: solutions}$$\n",
    "All solutions on the same Pareto front have the same fitness. \n",
    "\n",
    "#### Age-fitness Pareto survival\n",
    "\n",
    "A common problem in many applications of evolutionary algorithms is when the progress of the algorithm stagnates and solutions stop improving. Expending additional computational effort in the evolution often fails to make any substantial progress. This problem is known as premature convergence.   \n",
    "A common method for dealing with premature convergence is to perform many evolutionary searches, randomizing and restarting the search multiple times. This approach can be wasteful however, as the entire population is repeatedly thrown out. There is also the difficulty of deciding when to restart.  \n",
    "\n",
    "Age-fitness Pareto survival is a multi-objective method using age as an independent dimension in a multi-objective Pareto front optimization to favor new solutions that might perform better.\n",
    "\n",
    "The age of a solution is measured in generations. All randomly initialized individuals start with age of one. With each generation an individual exists in the population, its age is incremented by one. During crossover and mutation events, the age is inherited as the maximum age of the parents \n",
    "\n",
    "<img src=\"img/age_fitness_pareto.png\" width=\"400\">\n",
    "<center><a href='#Sources'>Source: Age-Fitness Pareto Optimization</a></center>\n",
    "\n",
    "#### Algorithm steps\n",
    "\n",
    "First, the population is initialized randomly.  \n",
    "At each generation, an extended population is created from:\n",
    "- the previous population\n",
    "- the mutation of randomly selected solutions\n",
    "- the children through crossover of randomly selected parents\n",
    "- new random solutions with age 0\n",
    "\n",
    "We introduce the *Age-Fitness Pareto fitness*, called `AFP_fitness`, computed from the previous `fitness` (the performance of a solution on a test dataset) and the `age` through a Non-Dominated Sorting algorithm. \n",
    "\n",
    "This *Age-Fitness Pareto fitness* is used to select the best solutions in the extended population to create the next population.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "We use inheritance to define a new classes `AFP_Program` from `Program` to add age, and`AFPPopulation` from `Population` that will implement Age-Fitness Pareto survival instead of tournament selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/afp_programs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load Code/afp_population.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run next cell to perform the process with Lexicase Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = AFPPopulation(data = yeast, init_size=15, pop_size=100)\n",
    "p.generation(10, plot=True)\n",
    "p.final_selection(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The work this notebook is based on implements the M4GP algorithm, evolution of $M_2GP$ (Multi-dimensional Multi-class Genetic Programming).   \n",
    "The best program generated by M4GP was compared to benchmark methods and other published results.\n",
    "\n",
    "The presented selection methods are also compared:\n",
    "- M4GP-tn: Tournament Selection\n",
    "- M4GP-lx: Lexicase selection\n",
    "- M4GP-ps: Age-fitness Pareto Survival (with a more advanced form than the algorithm presented in this notebook)\n",
    "\n",
    "The `Yeast` benchmark is the `Yeast` dataset presented in this notebook.\n",
    "\n",
    "<img src=\"img/benchmarks.png\" width=\"800\">\n",
    "<center><a href='#Sources'>Source: Multidimensional genetic programming for multiclass classification</a></center>\n",
    "\n",
    "M4GP represents an improvement over previous state-of-the-art techniques for multi-class classiﬁcation with GP (M2GP, M3GP and eM3GP).   \n",
    "It also outperformed classical solutions, including XGBoost and Deep Neural Networks, on biomedical applications:\n",
    "\n",
    "\n",
    "<img src=\"img/biomedical.png\" width=\"800\">\n",
    "<center><a href='#Sources'>Source: Multidimensional genetic programming for multiclass classification</a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources \n",
    "<a id='Sources'>  \n",
    "The pdf files of the sources used in this work are available in the `Source` repository.    \n",
    "</a>  \n",
    "\n",
    "Recommended readings:\n",
    "\n",
    "1.__Multidimensional genetic programming for multiclass classification__  \n",
    "La Cava, William & Silva, Sara & Danai, Kourosh & Spector, Lee & Vanneschi, Leonardo & Moore, Jason. (2018).     \n",
    "Swarm and Evolutionary Computation. 10.1016/j.swevo.2018.03.015.    \n",
    "   \n",
    "2.__M4GP__   \n",
    "La Cava, William & Silva, Sara & Vanneschi, Leonardo & Spector, Lee & Moore, Jason. (2017). Genetic Programming Representations for Multi-dimensional Feature Learning in Biomedical Classification. 158-173. 10.1007/978-3-319-55849-3_11. \n",
    "  \n",
    "3.__A Field Guide to Genetic Programming__  \n",
    "Riccardo Poli and William B. Langdon and Nicholas Freitag McPhee (2008)  \n",
    "Published via http://lulu.com and freely available at http://www.gp-field-guide.org.uk    \n",
    "  \n",
    "4.__Age-Fitness Pareto Optimization__  \n",
    "Schmidt, Michael & Lipson, Hod. (2010).   \n",
    "Age-Fitness Pareto Optimization. 8. 543-544. 10.1145/1830483.1830584. \n",
    "Available [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.375.6168&rep=rep1&type=pdf)\n",
    "  \n",
    "5.__A Novel Non-Dominated Sorting Algorithm for Evolutionary Multi-objective Optimization__   \n",
    "Bao, Chunteng & Xu, Lihong & Goodman, Erik & Cao, Leilei. (2017). A Novel Non-Dominated Sorting Algorithm for Evolutionary Multi-objective Optimization. Journal of Computational Science. 23. 10.1016/j.jocs.2017.09.015. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
